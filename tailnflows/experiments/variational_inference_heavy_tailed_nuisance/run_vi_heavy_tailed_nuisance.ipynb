{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boiler plate to allow running in colab, can ignore if running locally\n",
    "import subprocess\n",
    "try:\n",
    "    import tailnflows\n",
    "except ModuleNotFoundError:\n",
    "    # need to build environment\n",
    "    print('installing tailnflows environment...')\n",
    "    subprocess.run(['pip', 'install', 'git+https://github.com/Tennessee-Wallaceh/tailnflows'])\n",
    "    import tailnflows\n",
    "    from tailnflows.utils import configure_colab_env\n",
    "    configure_colab_env() # can take a while\n",
    "    import torch\n",
    "    torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tailnflows.models.flows import (\n",
    "    ModelUse, TTF_m, gTAF, mTAF, _df_to_tailp\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'TTF_m_fixed': lambda dim, dfs: TTF_m(\n",
    "        dim,\n",
    "        ModelUse.variational_inference,\n",
    "        model_kwargs=dict(\n",
    "            tail_bound=5.,\n",
    "            rotation=False,\n",
    "            num_bins=8,\n",
    "            pos_tail_init=_df_to_tailp(dfs),\n",
    "            neg_tail_init=_df_to_tailp(dfs),\n",
    "            fix_tails=True,\n",
    "        )\n",
    "    ),\n",
    "    'TTF_m': lambda dim, _: TTF_m(\n",
    "        dim,\n",
    "        ModelUse.variational_inference,\n",
    "        model_kwargs=dict(\n",
    "            tail_bound=5.,\n",
    "            rotation=False,\n",
    "            num_bins=8,\n",
    "        )\n",
    "    ),\n",
    "    'mTAF': lambda dim, dfs: mTAF(\n",
    "        dim,\n",
    "        ModelUse.variational_inference,\n",
    "        model_kwargs=dict(\n",
    "            tail_bound=5.,\n",
    "            rotation=False,\n",
    "            tail_init=torch.tensor(dfs),\n",
    "            num_bins=8\n",
    "        )\n",
    "    ),\n",
    "    'gTAF': lambda dim, _: gTAF(\n",
    "        dim, \n",
    "        ModelUse.variational_inference, \n",
    "        model_kwargs=dict(tail_bound=5., rotation=False, num_bins=8)\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th17628/miniconda3/envs/tailnflows/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== v=1.0, dim=5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 63.05it/s, elbo=-125.500, tst_ess=0.013, tst_psis=0.939, model=TTF_m_fixed]\n",
      "100%|██████████| 300/300 [00:04<00:00, 64.01it/s, elbo=-0.480, tst_ess=0.053, tst_psis=0.730, model=TTF_m]\n",
      "100%|██████████| 300/300 [00:04<00:00, 63.90it/s, elbo=-31.329, tst_ess=0.010, tst_psis=0.930, model=mTAF]\n",
      "100%|██████████| 300/300 [00:05<00:00, 56.21it/s, elbo=-0.788, tst_ess=0.001, tst_psis=0.821, model=gTAF]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== v=2.0, dim=5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 63.45it/s, elbo=-1.935, tst_ess=0.040, tst_psis=0.768, model=TTF_m_fixed]\n",
      "100%|██████████| 300/300 [00:05<00:00, 54.37it/s, elbo=-0.332, tst_ess=0.178, tst_psis=0.697, model=TTF_m]\n",
      "100%|██████████| 300/300 [00:05<00:00, 58.22it/s, elbo=-2.372, tst_ess=0.087, tst_psis=0.818, model=mTAF]\n",
      "100%|██████████| 300/300 [00:05<00:00, 54.05it/s, elbo=-0.276, tst_ess=0.213, tst_psis=0.837, model=gTAF]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== v=30.0, dim=5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:05<00:00, 59.37it/s, elbo=-0.170, tst_ess=0.822, tst_psis=0.365, model=TTF_m_fixed]\n",
      "100%|██████████| 300/300 [00:05<00:00, 58.27it/s, elbo=-0.338, tst_ess=0.542, tst_psis=0.550, model=TTF_m]\n",
      "100%|██████████| 300/300 [00:04<00:00, 60.64it/s, elbo=-0.122, tst_ess=0.353, tst_psis=0.794, model=mTAF]\n",
      "100%|██████████| 300/300 [00:05<00:00, 57.99it/s, elbo=-0.242, tst_ess=0.733, tst_psis=0.487, model=gTAF]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== v=1.0, dim=5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 61.73it/s, elbo=-2.346, tst_ess=0.001, tst_psis=0.713, model=TTF_m_fixed]\n",
      "100%|██████████| 300/300 [00:04<00:00, 62.16it/s, elbo=-0.318, tst_ess=0.005, tst_psis=0.713, model=TTF_m]\n",
      "100%|██████████| 300/300 [00:05<00:00, 52.51it/s, elbo=-79.737, tst_ess=0.023, tst_psis=0.878, model=mTAF]\n",
      "100%|██████████| 300/300 [00:06<00:00, 48.56it/s, elbo=-1.721, tst_ess=0.002, tst_psis=0.839, model=gTAF]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== v=2.0, dim=5 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 62.54it/s, elbo=-1.182, tst_ess=0.001, tst_psis=0.714, model=TTF_m_fixed]\n",
      " 36%|███▌      | 108/300 [00:02<00:03, 52.99it/s, neg_elbo=0.94, model=TTF_m]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(repeat)\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model_fcn(dim, dfs)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 20\u001b[0m losses, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheavy_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuisance_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m  \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m add_raw_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvi_newrun\u001b[39m\u001b[38;5;124m'\u001b[39m, label, (metrics\u001b[38;5;241m.\u001b[39mess, metrics\u001b[38;5;241m.\u001b[39mpsis_k, dim, nuisance_df))\n",
      "File \u001b[0;32m~/projects/tailnflows/tailnflows/train/variational_fit.py:47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, target, lr, num_epochs, batch_size, label, seed, grad_clamp)\u001b[0m\n\u001b[1;32m     45\u001b[0m x_approx, log_q_x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample_and_log_prob(batch_size)\n\u001b[1;32m     46\u001b[0m neg_elbo_loss \u001b[38;5;241m=\u001b[39m (log_q_x \u001b[38;5;241m-\u001b[39m target(x_approx))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 47\u001b[0m \u001b[43mneg_elbo_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/tailnflows/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tailnflows/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tailnflows.train.variational_fit import train\n",
    "from tailnflows.targets.heavy_tailed_nuisance import log_density\n",
    "from tailnflows.experiments.utils import add_raw_data\n",
    "import itertools\n",
    "\n",
    "# experiment parameters\n",
    "target_dims = [5]\n",
    "nuisance_dfs = [1., 2., 30.]\n",
    "repeats = 5\n",
    "experiment_params = itertools.product(range(repeats), target_dims, nuisance_dfs)\n",
    "\n",
    "for repeat, dim, nuisance_df in experiment_params:\n",
    "    print('=' * 20, f'v={nuisance_df}, dim={dim}', '=' * 20)\n",
    "    dfs = [nuisance_df] * dim # for shift data set all dfs are the same\n",
    "    for label, model_fcn in models.items():\n",
    "          torch.manual_seed(repeat)\n",
    "\n",
    "          model = model_fcn(dim, dfs).to(torch.float32)\n",
    "\n",
    "          losses, metrics = train(\n",
    "            model,\n",
    "            lambda x: log_density(x, heavy_df=nuisance_df),\n",
    "            lr=1e-3,\n",
    "            num_epochs=300,\n",
    "            batch_size=100,\n",
    "            label=label,\n",
    "            seed=repeat,\n",
    "          )\n",
    "\n",
    "          add_raw_data('vi_newrun', label, (metrics.ess, metrics.psis_k, dim, nuisance_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tailnflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
